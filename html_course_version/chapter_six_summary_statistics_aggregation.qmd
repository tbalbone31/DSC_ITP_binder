---
title: "Chapter 6 - Summary Statistics and Aggregation"
format:
  html:
    theme: default
    highlight: null
    toc: true
    toc-title: Contents
    toc-location: right
    toc-depth: 3
    number-sections: true 
    link-external-newwindow: true
---

![](../images/AF_DSC_banner.png){fig-alt="Government Analysis Function and Data Science Campus Logos."}

```{r, echo=FALSE, warning=FALSE, message=FALSE}

Sys.setenv(RETICULATE_PYTHON = "/Users/jakemarshall/Library/r-miniconda-arm64/envs/my_python/bin/python") # Set path for my venv

library(reticulate) # Initialise

library(kableExtra) # For tables
``` 

This is a HTML document. The Introduction to Python course is written and intended to be used in a Jupyter Notebook file. These HTML documents have been made available for users who require screen readers or other accessibility needs. These HTML documents have been tested, but if you notice any errors or any compatibility issues please contact us on the GSS Capability email inbox.

If you are using a screen reader you will need to set your punctuation level (sometimes called verbosity) to full, especially for the code sections.


*Chapter Overview and Learning Objectives*:

* [Packages and Data](#packages)
 - Packages
 - Data
 
* [Overall Descriptive Statistics](#describe)

* [Range](#range)
 - min
 - max
 - quantiles
 
* [Averages](#averages)
 - Mean
 - Median
 - Mode
 
* [Spread](#spread)
 - Standard Deviation
 - Variance

* [Counting Values](#count)
 - Counts
 - Null Value counts
 - Value Counts

* [Other Summary Statistics](#other)

* [Creating Size Bands](#cut)

* [Aggregation](#agg)

<a id='packages'></a>

# Packages and Datasets

## Packages
As a reminder, we should always import our packages at the top of our script. In this session we will use the following:

* pandas, and give it the nickname pd
* numpy and give it the nickname np

## Exercise
::: {.panel-tabset}
### **Packages**{-}
Import Pandas and give it the nickname pd
Import Numpy and give it the nickname np

### **Show Answer**{-}
```{python}
import pandas as pd
import numpy as np
```

:::

## Datasets
Good practice also dictates that we read in our datasets at the top of our script too.

In this session we’ll be using

| variable name | file name  |
| --- | --- | --- |
| animals | animals.csv |
| titanic | titanic.xlsx |

## Exercise
::: {.panel-tabset}
### **Load in the Data**{-}
Load in these datasets listed above

You can check your variables are loaded by using %whos in Jupyter. In Spyder or other IDE's they should appear in your variable explorer. 

### **Show Answer**{-}
```{python}
animals = pd.read_csv("../data/animals.csv")
titanic = pd.read_excel("../data/titanic.xlsx")
```

:::

If you struggle with this section, feel free to review the content of Chapter 3 that covered this in depth. Practicing these commands that we repeat over and over is a great way to retain the good practice guidelines. 

<a id='describe'></a>

# Overall Descriptive Statistics

Pandas has an inbuilt method to get basic descriptive statistics across our dataset, this is the .describe() method.

## Using the describe method

### Example

```{python eval = FALSE}
titanic.describe()
```

```{python include=FALSE }
display = titanic.describe()
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

We can also get summary statistics on a specific column.

```{python}
titanic["fare"].describe()
```

These statistics are explained in more detail below:
 
 | Summary Statistic | Description|
 |--------------------------|-----------------|
| count | the number (count) of non missing entries in the given column.
| mean | the average (arithmetic mean) data value in the given column.
| std | the standard deviation (spread) of values in the given column.
| min |  the smallest value in the given column.
| 25% | the value of the data at the lower quartile <br> (i.e. after the first 25% of data, ordered from smallest to largest).
| 50% | the middle value of the data (aka the median). <br> half the values are larger than this value, and half smaller.
| 75% | the value of the data at the upper quartile <br> (i.e. after the first 75% of data, ordered from smallest to largest).
| max | the maximum data value recorded.

### Example 2

Describe works across all numeric columns by default. We can display descriptive information for other data types by using the parameter include=   

This parameter takes a list as the input; even if we’re just including one kind of data. Here we’re specifying that we want to include “object”, our text/string columns.

```{python eval = FALSE}
titanic.describe(include=["object"])
```

```{python include=FALSE }
display = titanic.describe(include=["object"])
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

 | Summary Statistic | Description|
 |--------------------------|-----------------|
| count | The number (count) of non missing entries in the given column.
| unique | The number of unique variables in a column.
| top | The most frequently occurring value.
| freq | The frequency of the “top” value (how often it appears).

If there are two or more “top” values; e.g. both most frequently occurring values that have the same frequency within the table, Python will [kind of arbitrarily](https://github.com/pandas-dev/pandas/issues/15833) choose one of them to be the top value. The link is added for general interest, you don't need to understand it.

In this data there are two women and two men who share the same name. Pandas will choose one of them to display.

<a id='range'></a>

# Range

We can also access these summary statistics individually. In most cases the name of the method is the same as the summary statistic.

## min

We can use .min() to return the minimum value in a column.

### Example

```{python}
titanic["fare"].min()
```

This also works for object (text) columns.

```{python}
titanic["name"].min()
```

## max

We can use .max() to return the maximum value in a column.

### Example

```{python}
titanic["fare"].max()
```

This also works for object (text) columns. 

```{python}
titanic["name"].max()
```

Something important to note here is that pandas effectively assigns a value to each letter. This goes A-Z and **then** a-z. So a lower case "a" is treated as coming **after** a capital "Z" in Python.

This is why van Melkebeke, Mr. Philemon is the maximum value in our Titanic Dataframe rather than Zimmerman, Mr Leo. We can handle this issue by ensuring our data is either all lower case or all upper case before finding the .min() or .max() values.

### Example

Here I have handled the issue by chaining the methods .str.lower() and .max() together. If I wanted to do more work with this column in future I may consider overwriting it with a lower case version (which would form part of the cleaning routine).

```{python}
titanic["name"].str.lower().max()
```
The chaining of methods here applies left to right just as it is read, the column is selected, then the .lower() method is used from `str` methods, and then the .max() method is applied to the lower case column.

## Quantiles

We can use .quantile() to find information at different points of our data.

### Example

Our parameter is q=  and then a decimal number. If we don't specify this the default behaviour is 0.5, which returns the median (as it is the 50% quantile).

```{python}
titanic["fare"].quantile(q=0.25)
```

If we wish to specify more than one, we pass a list to the parameter q= 

```{python}
titanic["fare"].quantile(q=[0, 0.25, 0.5, 0.75, 1])
```
This is a great time to mention that you don't necessary need to type a full list of numbers this way, plus the numbers we may want to use could belong to an enormous list, which is unreasonable to type by hand. This is where the range functions come in. 

There are two of these functions:

* range(start=, stop=, step=) where the start parameter specifies where to start, stop specifies where to stop (which is **exclusive**! Not included) and step is how we should jump from number to number, this **must** be an integer.

* np.arange(start=, stop=, step=) comes from the numpy package (np) and creates **a** range from the start to the stop values (where the stop is **exclusive**). However, this function allows us to step in increments of any size, including floats (or decimal numbers).

Let's see two quick examples before using this with the quantile() function. 

### Example 

```{python}
# Creating a range object 

x = range(0, 10, 2) # Jump from 0-10 in increments of 2

# Using np.arange() to obtain an array of values

y = np.arange(0, 10, 0.5) # Jump from 0-10 in increments of 0.5

print(x, y)
```

You see here that the range() function gives a range object, rather than a list like structure of the values themselves, this restricts its use even further as it is less readable. It is recommended that np.arange() is used in most circumstances. Whilst the second output looks like a list, it is actually an **array**, a numpy specific object that is beyond the scope of this course. 

For more information consult this [link](https://numpy.org/doc/stable/user/absolute_beginners.html) from Numpy. In the future, there will be some **Reference Material** added to this course that introduces this data type, as it is a great piece of material to come back to when moving onto more complex causes like **Introduction to Machine Learning**. This will never be listed as Core material, so don't concern yourself too much with it. 

## Example 2

Let's see how this function helps when finding quantiles.

```{python}
# Note that .arange() comes from the numpy package (np)


titanic["fare"].quantile(q=np.arange(start=0.0, 
                                     stop=1.1,  # Remember this is exclusive!
                                     step=0.1)) 
```


## Exercise
::: {.panel-tabset}
### **Exercise**{-}
(a) How old are the youngest and oldest passengers in the titanic DataFrame?
(b) Print out the 20th, 40th, 60th and 80th percentiles of the fare column in the titanic DataFrame.

### **Show Answer**{-}

```{python}
# (a)

max_age = titanic["age"].max()

min_age = titanic["age"].min()

print("The oldest person on the titanic was", max_age, "and the youngest person was", str(min_age)+".")
```
```{python}
# (b)

percentiles = titanic["fare"].quantile(q = np.arange(start = 0.2,
                                                     stop = 1,
                                                     step = 0.2))
percentiles
```
:::


<a id='averages'></a>

# Averages

Now we will look at Measures of Central Tendency, often referred to as Averages. 

## Mean

Seen briefly in the previous chapter when discussing filling in missing values, the .mean() method (this is the **arithmetic mean**, which corresponds to summing all values and then dividing by the  number of values there were) computes the arithmetic mean of a column.

### Example

```{python}
# Compute mean of the fare column

titanic["fare"].mean()
```

## Exercise
:::{.panel-tabset}
### **Exercise**{-}
Confirm that the value given by .mean() for the fare column is the same as computing it manually. 

Hint: You will need the .sum() function to compute the total of the column and the .count() function to compute the number of values (we will see this function in some more detail later). Remember that to compare if two objects are equal, we need to use ==.

### **Show Answer**{-}
```{python}
mean_way_one = titanic["fare"].mean()

mean_way_two = titanic["fare"].sum()/titanic["fare"].count()

# Remember to compare two objects we use the == logical operator to return True or False

mean_way_one == mean_way_two
```

An interesting observation here is the using len() won't work here and provide an answer that is slightly out, this is because len() counts all values (including nulls), whereas .count() only counts the non-null values (and is what is used by .mean() in the background).

:::

## Median

The .median() is the middle value (50th percentile) when the numbers are listed in order.

### Example

```{python}
titanic["fare"].median()
```

## Mode

The .mode() is the value that occurs most frequently in the column.

### Example 1

```{python}
titanic["fare"].mode()
```

### Example 2

We can also find the mode in object based columns. Here the name column has two modes, since both "Connolly, Miss. Kate" and "Kelly, Mr. James" appear twice in the Data Frame.

Generally, when these cases appear the following terminology is used:

* One mode - Unimodal
* Two modes - Bimodal
* Three modes - Trimodal 

As such, the name column is bimodal in this example. If you're wondering how we know the frequency (the number of times they each appear in the DataFrame), we will see a very useful method for this later in the chapter.  

```{python}
titanic["name"].mode()
```

Interestingly this data isn’t duplicates! There really were two separate individuals with those names!

```{python eval = FALSE}
titanic[(titanic["name"] == "Connolly, Miss. Kate") | (titanic["name"] == "Kelly, Mr. James")]
```

```{python include=FALSE }
display = titanic[(titanic["name"] == "Connolly, Miss. Kate") | (titanic["name"] == "Kelly, Mr. James")]
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```


<a id='spread'></a>

# Spread

Another element of the data we often want to investigate is how spread out it is, we use Measures of Spread to do so. 

## Standard Deviation

The standard deviation measures the spread of the data about the mean value. It shows you how much your data distribution is spread out around the mean or average.

### Example

We can calculate the standard deviation using the .std() method.

```{python}
titanic["fare"].std()
```

## Variance

Variance measures how spread out the values of a variable are in general.

### Example

We can calculate the variance using the .var() method.

```{python}
titanic["fare"].var()
```

## Exercise
::: {.panel-tabset}
### **Exercise**{-}

(a)(i) Find the variance and standard deviation of the age column in the titanic dataset.

(a)(ii) Round these two values to 2 decimal places and output the values in a sentence.

(b) There is a link between these two metrics, namely that the Standard Deviation is the square root of the Variance. Confirm this relationship. Hint: To use square root, we need the np.sqrt() function from numpy (import numpy as np). 

### **Show Answer**{-}
```{python}
# (a)(i) & (a)(ii)

var_age = round(titanic["age"].var(), ndigits = 2)

std_age = round(titanic["age"].std(), ndigits = 2)

print("The variance of the ages of titanic passengers was", var_age, "and the standard deviation was", str(std_age) + ".")
```

```{python}
# (b) 

var_age = titanic["age"].var()

std_age = titanic["age"].std()

np.sqrt(var_age) == std_age
```

:::


<a id='count'></a>

# Counting Values

## Counts

As we saw in the earlier exercise, we can find the number of non null values in a column using the .count() method. By non null values we mean values with data in them, not the missing values.

As a reminder we can use either .shape or len() to find the number of rows, which for titanic is 1309 rows (this includes rows that have missing values).

### Example

```{python}
titanic["embarked"].count()
```

As you can see embarked returns 1307 - so there's 2 missing values.

## Null Value counts

We can find how many null values we have by using the .isnull() method. This returns a Boolean series consisting of True and False values whether the value is null or not. 

### Example 1

```{python}
titanic["age"].isnull().tail()
```

As these have numeric values behind them (True is 1, False is 0 ) we can use .sum() to total them and obtain the number of null values in a column.

```{python}
titanic["age"].isnull().sum()
```

In more modern versions of Pandas the method .isna() also exists and provides the same output.

## Exercise
::: {.panel-tabset}
### **Example**{-}
 Compute the proportion of null values (a decimal between 0 and 1, equivalent to a percentage, for example 0.85 means 85%) in the age column. Use the isna() function to compute the number of null values this time.
 
Hint: To compute this, we need to first compute the number of null values and divide by the number of values overall in the column **including the nulls!**.

### **Show Answer**{-}

```{python}
# Proportion of null values

(titanic["age"].isna().sum())/len(titanic["age"])
```
So we see that the age column has 20% missing values.

:::

## Value Counts

We can find the frequencies of each unique value in a column, by using .value_counts(). This is particularly useful when applied to object columns to observe what values appear the most and least frequently.

```{python}
titanic["sex"].value_counts()
```

## Exercise 
::: {.panel-tabset}
### **Exercise**{-}

(a) How many passengers were in each class?

(b) Look in the help for pd.Series.value_counts() to see how you can return the values as a proportion instead. 

### **Show Answer**{-}
```{python}
# (a)

titanic["pclass"].value_counts()
```

```{python}
# (b)

# Use the parameter and argument  normalize = True to return proportions

# Show the help documentation

help(pd.Series.value_counts)

# modify the previous code
titanic["pclass"].value_counts(normalize=True)
```
:::

<a id='other'></a>

# Other Summary Statistics

## Sum

We can use .sum() to add up columns of numeric data, which we saw when counting the null values. 

### Example 1

```{python}
titanic["fare"].sum()
```

The .sum() method comes from Pandas; there is also an inbuilt function sum(). However if we have null values in the column this will return us nan.

### Example 2

```{python}
sum(titanic["fare"])
```

## Unique

We can use .unique() to find values that are unique in a column. This is returned as an array.

### Example

```{python}
titanic["boat"].unique()
```

## Nunique

.nunique() can be used to find the number of unique values in a column.

### Example

```{python}
titanic["boat"].nunique()
```


<a id='cut'></a>

# Creating Size Bands

## pd.cut()

We can use the method pd.cut() to cut or “bin” our data into groups or categories. This is commonly done when creating size bands; like age bands, to create a categorical column out of a numeric one. 

The method pd.cut() takes a column of data and groups it into “bins” or categories. This column will have the data type of “category”. There’s some more information about the “category” data type [in this link]( https://pbpython.com/pandas_dtypes_cat.html).

We often assign the output of pd.cut() to a new column. This is because we don’t want to overwrite and change the data type of the existing column.

### Example

```{python eval = FALSE}
titanic["binned_ages"] = pd.cut(titanic["age"],
                                bins=10)

titanic.head()
```

```{python include=FALSE }
titanic["binned_ages"] = pd.cut(titanic["age"],
                                bins=10)
titanic["binned_ages"] = titanic["binned_ages"].astype("str")
display = titanic.head()
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

We set the parameter bins = to specify the number of categories that we want. By passing an integer to this, Pandas takes the smallest value and the largest value in the column and creates the number of categories defined.

We can look at our bins. Note the bracket denotes **exclusion** of that number and the square bracket denotes **inclusion** of that number. 

### Example 1

Now these are categories we can see there is a relationship between each category.

```{python}
titanic["binned_ages"].unique()
```

We can also pass our own values to determine where the edges of the categories are, rather than allowing Python to compute these approximate ones. This could be as a list of values or a range object that we discussed before. 

### Example 2

Note that here I am having to use the numpy method np.arange() as there will be decimals. 

```{python eval = FALSE}
titanic["binned_ages2"]  = pd.cut(titanic["age"],  # Data to cut
                                  bins=np.arange(start=0,
                                                 stop=(titanic["age"].max() + 1) , # Remember stop is exclusive!
                                                 step=10))

titanic.head()
```

```{python include=FALSE }
titanic["binned_ages2"]  = pd.cut(titanic["age"],  # Data to cut
                                  bins=np.arange(start=0,
                                                 stop=(titanic["age"].max() + 1) , # Remember stop is exclusive!
                                                 step=10))
titanic["binned_ages2"] = titanic["binned_ages2"].astype("str")
display = titanic.head()
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

This means I don’t have to know the maximum value for the column before I write this piece of code. You can see how we can combine methods quite easily to make our life easier when performing such tasks.

It is important to note that in this part of the code :

stop=(titanic["age"].max() + 1)

I use brackets to enforce the order of operations. If I didn’t add 1 to the stop, the entry at my maximum value would read NaN, which I want to avoid.

### Example 3

We can also add labels to our categories. This time rather than displaying the bin edges it will display the text strings we specify.

This is passed as a list to the parameter labels= to the pd.cut() function.

```{python eval = FALSE}
titanic["binned_ages3"] = pd.cut(titanic["age"],  # Data to cut
                                 bins=np.arange(start=0,
                                                        stop=(titanic["age"].max() + 1),
                                                step=10),
                                 labels=["0 – 10", "11-20", " 21 – 30",
                                         "31 – 40", "41 – 50", "51 – 60",
                                         "61 – 70", "71 - 80"])

titanic.head()
```

```{python include=FALSE }
titanic["binned_ages3"] = pd.cut(titanic["age"],  # Data to cut
                                 bins=np.arange(start=0,
                                                stop=(titanic["age"].max() + 1),
                                                step=10),
                                 labels=["0 – 10", "11-20", " 21 – 30",
                                         "31 – 40", "41 – 50", "51 – 60",
                                         "61 – 70", "71 - 80"])
titanic["binned_ages3"] = titanic["binned_ages3"].astype("str")
display = titanic.head()
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

Note these bands are approximate, e.g someone with an age of 20.2 will go into the band labelled "21-30". Integers were chosen as it's an easier read, and most ages after 1 are whole numbers. 

There’s additional parameters we can set here; check the help function if there’s anything specific you need to do. You will get some more practice with this in the Case Study in Chapter 7. 

## pd.qcut()

pd.qcut() is described in the documentation as a “Quantile-based discretization function”. This means that pd.qcut() tries to divide the data into bins of equal sizes, rather than using the numeric edges of the data to create the bins.

### Example

In the cell below I’m using pd.qcut() to cut the age column in to 10. This using the same data and the same number of segments as the pd.cut() we did at the start.

```{python eval = FALSE}
# Divide fare into 3 equally sized classes.
titanic["age_qcut"] = pd.qcut(titanic["age"],
                              q=10)  # q for quantiles

#View the data
titanic.head()
```

```{python include=FALSE }
# Divide fare into 3 equally sized classes.
titanic["age_qcut"] = pd.qcut(titanic["age"],
                              q=10)  # Note the parameter here is q
titanic["age_qcut"] = titanic["age_qcut"].astype("str")
display = titanic.head()
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

## Visual Differences between cut and qcut

We can really see the difference between the two new "cut" columns if we visualise them. In the following, I've taken a pd.cut() and a pd.qcut() and set bins/q = 10.

* The pd.cut() action on the left, cuts the range of the data into 10 bins. You can see the data is not distributed evenly between these 10 bins, but the bins are of equal size.

* The pd.qcut() action on the right cuts the data so each of the 10 bins has roughly an equal number in each bin (edge cases in this case have made it slightly more uneven as we cannot split identical values between bins!). You can see the size of bins is not uniform, unlike the pd.cut() bins.


![A comparison of cut and qcut](../images/comparing_cut_and_qcut.svg){fig-alt="A comparison of cut and qcut and the differences between them."}

## Chart Code
::: {.panel-tabset}
### **Chart Code**{-}

Note that this course doesn't cover plotting, there is a **Data Visualisation in Python** course avalible on the Learning Hub. The code is provided here for information purposes only.

### **Show Code**{-}
```{python, eval = F}
# Import the package matplotlib - and use the magic command to show visualisations below the cell.

import matplotlib.pyplot as plt
#% matplotlib inline

# Prep The Data for Visualsation with pd.cut() 

titanic["binned_ages_2"]  = pd.cut(titanic["age"],  
                                   bins=10,
                                   labels=["0.0869 to 8.15", "8.16 to 16.133", "16.134 to 24.117", "24.117 to 32.1",
                                           "32.2 to 40.083", "40.084 to 48.067", "48.068, 56.05", "56.06 to 64.033" ,
                                           "64.033 to 72.017", "72.017, 80.0"])

# Find the value counts, set the columns in the order I want and reset the index

pd_cut_values = titanic["binned_ages_2"].value_counts()

pd_cut_values = pd_cut_values.reindex(["0.0869 to 8.15", "8.16 to 16.133", "16.134 to 24.117", "24.117 to 32.1",
                                       "32.2 to 40.083", "40.084 to 48.067", "48.068, 56.05", "56.06 to 64.033" , 
                                       "64.033 to 72.017", "72.017, 80.0"])

pd_cut_values = pd_cut_values.reset_index()


# Prep The Data for Visualsation using pd.qcut()

titanic["q_cut_ages"] = pd.qcut(titanic["age"],
                                q=10,
                                labels=["0.166 to 14.0", "14.1 to 19.0", "19.1 to 22.0", "22.0 to 25.0", "25.0 to 28.0", 
                                        "28.1 to 31.0", "31.1 to 36.0", "36.0 to 42.0", "42.0, 50.0", "50.0 to 80.0"])

# Find the value counts and reset the index

q_cut_values = titanic["q_cut_ages"].value_counts()

q_cut_values = q_cut_values.reindex(["0.166 to 14.0", "14.1 to 19.0", "19.1 to 22.0", "22.0 to 25.0", "25.0 to 28.0", 
                                     "28.1 to 31.0", "31.1 to 36.0", "36.0 to 42.0", "42.0, 50.0", "50.0 to 80.0"])

q_cut_values = q_cut_values.reset_index()

# Set up the figure and axes (which are objects!)

f, (ax1, ax2) = plt.subplots(1,2,figsize=(16,6))

#Plot Figure 1 - our pd.cut()

ax1.barh(np.arange(len(pd_cut_values)), pd_cut_values["binned_ages_2"], color="#094267ff")
ax1.set_xlabel("Frequency")
ax1.set_ylabel("Bins")
ax1.set_title("Using pd.cut() - The Frequency of Data Per Bin")
ax1.set_yticklabels(pd_cut_values["index"])
ax1.set_yticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# Plot figure 2 - our pd.qcut()

ax2.barh( np.arange(len(q_cut_values)), q_cut_values["q_cut_ages"], color="#a4cf00ff")
ax2.set_xlabel("Frequency")
ax2.set_ylabel("Bins")
ax2.set_title("Using pd.qcut() - The Frequency of Data Per Bin")
ax2.set_yticklabels(q_cut_values["index"])
ax2.set_yticks([0,1,2,3,4,5,6,7,8,9,10])

# Space out the figures so the right y_label doesn't overlap!

f.tight_layout(rect=[0, 0.03, 1, 0.95])

# Set a Centered title to for both figures.

f.suptitle("Comparing the distribution of data within bins using pd.cut() and pd.qcut()") ;
```

:::


<a id='agg'></a>

# Aggregation

Aggregation means grouping data together by a particular grouping variable (of our choice) and producing a summary of one or more columns for that grouping variable. For example, what if I want to find out if the average fare paid is different for each passenger class? I could use this technique to find out. 

We have 3 passenger classes; 1st, 2nd and 3rd, which we can check that using the .unique() function as we saw earlier.

```{python}
titanic["pclass"].unique()
```

## Introducing the groupby method

We'll use the .groupby() method in this tutorial. Panda's also provides us with the pd.crosstab() and the pd.pivot_table() methods as well. You can find these in the **Reference Material** as they are a little more niche in their application. 

This function can be really useful, especially when your data are disaggregate, e.g. data about individual units of people or things, rather than by category. 

The method .groupby() allows us to aggregate by a categorical variable and summarise numerical data into a new DataFrame. It works on a very interesting principle known as **split-apply-combine**:

![image showing the stages of a group by](../images/group_by.JPG){fig-alt="Image showing the stages of the groupby, split then apply then combine."}

* Split is where a DataFrame is divided into a set of smaller DataFrames based on the grouping variable. 

* Apply is where an aggregation is applied to each of the groups to create a single row for each group in the original DataFrame (for example computing a mean).

* Combine is where we bring together the aggregated DataFrame rows into a final new DataFrame.

### Example 1

Let's see an example of this in action, it sounds complex (and it is with what goes on behind the scenes), but the final output is much more readable.

```{python eval = FALSE}
titanic_class_fare = titanic.groupby(by="pclass")["fare"].mean() 

titanic_class_fare
```

```{python include = FALSE}
titanic_class_fare = titanic.groupby(by="pclass")["fare"].mean() 

display = titanic_class_fare.reset_index()

```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

I want to find if the .mean() value of the Fare was different depending where someone embarked. let's break the code down:

* In the .groupby() method to the by=  parameter I pass the column I wish to group by. The column pclass has three values,  "1" , "2" and "3".

* The .groupby() behaviour will effectively split the original DataFrame titanic into three new DataFrames. One with the values of 1, one for 2 and one with the values of 3. This is the **split** step.

* From these new .groupby() DataFrames I select the column ["fare"] and apply the summary statistic .mean() to it. This is the **apply** step.

* This is returned in the DataFrame “titanic_class_fare”, this is the **combine** step.

### Example 2

We can also use more complicated groupings using more than one variable, here we group first by pclass then embarked. As with many methods we discussed in earlier chapters, we must specify the column names in a list, where order is important. 

```{python eval = FALSE}
titanic.groupby(by=["pclass", "embarked"])["fare"].mean()
```

```{python include = FALSE}
titanic_gb = titanic.groupby(by=["pclass", "embarked"])["fare"].mean()

display = titanic_gb.reset_index()
display
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

To show off why order is important, see the following:

```{python eval = FALSE}
titanic.groupby(by=["embarked", "pclass"])["fare"].mean()
```

```{python include = FALSE}
titanic_gb = titanic.groupby(by=["embarked", "pclass"])["fare"].mean()

display = titanic_gb.reset_index()
display
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

### Example 3

You can also use other summary statistics here, for example .count() to return the number of values. The following shows that 141 passengers embarked in Cherbourg (embarked = C) and were pclass 1.

```{python eval = FALSE}
titanic.groupby(by=["embarked", "pclass"])["fare"].count()
```

```{python include = FALSE}
titanic_gb = titanic.groupby(by=["embarked", "pclass"])["fare"].count()

display = titanic_gb.reset_index()
display
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```


Now let's do some exercises!

## Exercise
::: {.panel-tabset}
### **Exercise**{-}

(a) Group animals by the column AnimalClass and find the .sum() of the IncidentNominalCost(£) column.

(b) Group animals by the column Borough and AnimalClass and find the .mean() of the PumpHoursTotal column.

(c) Reverse the order of the grouping in (b) and observe the differences. 

### **Show Answer**{-}

```{python eval = FALSE}
# (a)

animalclass_sum = animals.groupby(by="AnimalClass")['IncidentNominalCost(£)'].sum()

animalclass_sum
```

```{python include = FALSE}
animalclass_sum = animals.groupby(by="AnimalClass")['IncidentNominalCost(£)'].sum()

display = animalclass_sum.reset_index()
display
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

```{python eval = FALSE}
# (b) 

pump_hours_mean = animals.groupby(by=["Borough", "AnimalClass"])['PumpHoursTotal'].mean()

pump_hours_mean.head()
```

```{python include = FALSE}
pump_hours_mean = animals.groupby(by=["Borough", "AnimalClass"])['PumpHoursTotal'].mean()

display = pump_hours_mean.reset_index()
display
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

```{python eval = FALSE}
# (c) 

pump_hours_mean = animals.groupby(by=["AnimalClass", "Borough"])['PumpHoursTotal'].mean()

pump_hours_mean.head()
```
:::

```{python include = FALSE}
pump_hours_mean = animals.groupby(by=["AnimalClass", "Borough"])['PumpHoursTotal'].mean()

display = pump_hours_mean.reset_index()
display
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

## Multiple Aggregation

If we want to return more than one aggregation at once to the columns we've selected, there is the .agg() method. This takes a dictionary where the key is our column and the value is the aggregation method we wish to apply as a string.

These aggfuncs are slightly different to what we’ve seen, but are straightforward like “sum”, ”count”, ”mean” etc. Let's first see it with one aggregation before building up to two. 

### Example 1

```{python eval = FALSE}
animalclass_sum = animals.groupby(by="AnimalClass").agg({"IncidentNominalCost(£)": "sum",
                                                         "PumpHoursTotal": "mean"})

animalclass_sum
```

```{python include = FALSE}
animalclass_sum = animals.groupby(by="AnimalClass").agg({"IncidentNominalCost(£)": "sum",
                                                         "PumpHoursTotal": "mean"})

display = animalclass_sum.reset_index()

```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

If we want to apply more than one aggregation to a column we can pass a list to the values of the dictionary. This requires us to use the numpy methods which are very similar to the methods we have used previously, so np.sum , np.mean etc.

### Example 2

Here we will output the sum and the mean for IncidentNominalCost(£) and the mean for PumpHoursTotal. Notice that since there is only one method applied in the second case, we can just use the standard mean method, it is only when we use lists that numpy comes into play.

```{python eval = FALSE}
animalclass_sum = animals.groupby(by="AnimalClass").agg({"IncidentNominalCost(£)": [np.sum, np.mean],
                                                         "PumpHoursTotal": "mean"})

animalclass_sum
```

```{python include = FALSE}
animalclass_sum = animals.groupby(by="AnimalClass").agg({"IncidentNominalCost(£)": [np.sum, np.mean],
                                                         "PumpHoursTotal": "mean"})

display = animalclass_sum.reset_index()

```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

## Exercise 
::: {.panel-tabset}
### **Exercise**{-}

Group the animals dataset by the variable Borough and obtain the following summary statistics:

* The variance and standard deviation of the IncidentNominalCost(£) column.

* The mean of the PumpCount column.

* The sum of the PumpHoursTotal column.

### **Show Answer**{-}

```{python eval = FALSE}
animals_aggregates = animals.groupby(by = "Borough").agg({"IncidentNominalCost(£)":[np.var, np.std],
                "PumpCount":"mean",
                "PumpHoursTotal":"sum"})

animals_aggregates
```

```{python include = FALSE}
animals_aggregates = animals.groupby(by = "Borough").agg({"IncidentNominalCost(£)":[np.var, np.std],
                "PumpCount":"mean",
                "PumpHoursTotal":"sum"})

display = animals_aggregates.reset_index()
```

```{r echo=FALSE}
kable(py$display) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```
:::

In later versions of Pandas (from 0.25.0) named aggregation also exists. [A great guide to this can be found here](https://deanla.com/pandas_named_agg.html). We don't cover this here formally as it requires us to nest dictionaries which is outside the scope of the course but is definitely worth picking up once you have had more time with Python.

Note that these DataFrames displayed when you run the code look a little different to the ones I have displayed here. You will see that in yours, the index is our grouping categories. We can use our .reset_index() method here too, or use "as_index = False". There are a few reasons we might do this, including making visualisation easier.

```{python}
group_by_dropped_index = titanic.groupby(by=["embarked", "pclass"], as_index= False)["fare"].count()
group_by_dropped_index
```

```{r echo=FALSE}
kable(py$group_by_dropped_index) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  scroll_box(width = "100%")
```

<a id='END'></a>

# Chapter Summary

Excellent job, you have completed Chapter 6 of the Introduction to Python course. There is only one chapter left before you complete **Core Part 1** of the course! Absolutely amazing aggregating!

Chapter 7 will feature a structured end of course exercise, that tests you on everything you have learned from Chapter 3 to Chapter 6, with a new dataset we have yet to explore, the **schools** dataset. 
